{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5fc15a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252c31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Retrieve the dataset from this link and assign it to a variable â€˜chipoâ€™.\n",
    "# Note: You will be using â€˜chipoâ€™ in future exercises in the XP Gold and XP Ninja sections.\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv\"\n",
    "chipo = pd.read_csv(url, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce9fee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  quantity                              item_name  \\\n",
      "0         1         1           Chips and Fresh Tomato Salsa   \n",
      "1         1         1                                   Izze   \n",
      "2         1         1                       Nantucket Nectar   \n",
      "3         1         1  Chips and Tomatillo-Green Chili Salsa   \n",
      "4         2         2                           Chicken Bowl   \n",
      "5         3         1                           Chicken Bowl   \n",
      "6         3         1                          Side of Chips   \n",
      "7         4         1                          Steak Burrito   \n",
      "8         4         1                       Steak Soft Tacos   \n",
      "9         5         1                          Steak Burrito   \n",
      "\n",
      "                                  choice_description item_price  \n",
      "0                                                NaN     $2.39   \n",
      "1                                       [Clementine]     $3.39   \n",
      "2                                            [Apple]     $3.39   \n",
      "3                                                NaN     $2.39   \n",
      "4  [Tomatillo-Red Chili Salsa (Hot), [Black Beans...    $16.98   \n",
      "5  [Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...    $10.98   \n",
      "6                                                NaN     $1.69   \n",
      "7  [Tomatillo Red Chili Salsa, [Fajita Vegetables...    $11.75   \n",
      "8  [Tomatillo Green Chili Salsa, [Pinto Beans, Ch...     $9.25   \n",
      "9  [Fresh Tomato Salsa, [Rice, Black Beans, Pinto...     $9.25   \n"
     ]
    }
   ],
   "source": [
    "# 3. Display the first 10 rows of â€˜chipoâ€™ using the head() function.\n",
    "\n",
    "print(chipo.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a721cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries (rows) in 'chipo': 4622\n"
     ]
    }
   ],
   "source": [
    "# 4. Determine the total number of entries (rows) in â€˜chipoâ€™ using shape or info().\n",
    "total_rows = chipo.shape[0]\n",
    "print(\"Total number of entries (rows) in 'chipo':\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "112a9823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of columns in 'chipo': 5\n"
     ]
    }
   ],
   "source": [
    "# 5. se shape or columns to find the total number of columns in â€˜chipoâ€™.\n",
    "total_columns = chipo.shape[1]\n",
    "print(\"Total number of columns in 'chipo':\", total_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9276d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "Index(['order_id', 'quantity', 'item_name', 'choice_description',\n",
      "       'item_price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 6. Use columns to print all the column names.\n",
    "print(\"Column names:\")\n",
    "print(chipo.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fb17754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=4622, step=1)\n"
     ]
    }
   ],
   "source": [
    "# 7. Utilize index to understand how the DataFrame is indexed.\n",
    "print(chipo.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac12fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most ordered item: Chicken Bowl\n"
     ]
    }
   ],
   "source": [
    "# 8. Use the value_counts() method on the â€˜item_nameâ€™ column to find the most ordered item.\n",
    "\n",
    "most_ordered_item = chipo['item_name'].value_counts().idxmax()\n",
    "print(\"\\nMost ordered item:\", most_ordered_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0e10592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of items ordered: 4972\n"
     ]
    }
   ],
   "source": [
    "# 9. Find the total number of items ordered by summing the â€˜quantityâ€™ column.\n",
    "total_items_ordered = chipo['quantity'].sum()\n",
    "print(\"Total number of items ordered:\", total_items_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c98d0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most ordered item: [Diet Coke]\n"
     ]
    }
   ],
   "source": [
    "# 10. Use value_counts() on â€˜choice_descriptionâ€™ to find the most ordered item from this column.\n",
    "most_ordered_choice = chipo['choice_description'].value_counts().idxmax()\n",
    "print(\"Most ordered item:\", most_ordered_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f89a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Convert â€˜item_priceâ€™ to a float datatype using apply() and lambda.\n",
    "chipo['item_price'] = chipo['item_price'].apply(lambda x: float(x[1:]) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b6a8619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue: 34500.16\n"
     ]
    }
   ],
   "source": [
    "# 12. Calculate the total revenue for the dataset by summing the â€˜item_priceâ€™ column.\n",
    "total_revenue = chipo['item_price'].sum()\n",
    "print(\"Total revenue:\", total_revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02883ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of orders: 1834\n"
     ]
    }
   ],
   "source": [
    "# 13. Find the total number of orders by using the nunique() function on the â€˜order_idâ€™ column.\n",
    "total_orders = chipo['order_id'].nunique()\n",
    "print(\"Total number of orders:\", total_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72521d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average order value: 18.811428571428575\n"
     ]
    }
   ],
   "source": [
    "# 14. Compute the average order value by dividing the total revenue by the number of orders.\n",
    "average_order_value = total_revenue / total_orders\n",
    "print(\"Average order value:\", average_order_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1223c3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique items sold: 50\n"
     ]
    }
   ],
   "source": [
    "# 15. Determine the total number of unique items sold using the nunique() function on the â€˜item_nameâ€™ column.\n",
    "total_unique_items = chipo['item_name'].nunique()\n",
    "print(\"Total number of unique items sold:\", total_unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68b94e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of products costing more than $10: 1130\n"
     ]
    }
   ],
   "source": [
    "# ðŸŒŸExercise 2: Filtering & Sorting (Chipo)\n",
    "# Use the already created dataframe â€˜chipoâ€™ from the previous section.\n",
    "# Using conditional selection, find the count of products that cost more than $10. \n",
    "# Note: the price is listed in the â€˜item_priceâ€™ column.\n",
    "count_products_above_10 = chipo[chipo['item_price'] > 10]['item_name'].count()\n",
    "print(\"Count of products costing more than $10:\", count_products_above_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3aafd7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occupation\n",
      "administrator    38.746835\n",
      "artist           31.392857\n",
      "doctor           43.571429\n",
      "educator         42.010526\n",
      "engineer         36.388060\n",
      "entertainment    29.222222\n",
      "executive        38.718750\n",
      "healthcare       41.562500\n",
      "homemaker        32.571429\n",
      "lawyer           36.750000\n",
      "librarian        40.000000\n",
      "marketing        37.615385\n",
      "none             26.555556\n",
      "other            34.523810\n",
      "programmer       33.121212\n",
      "retired          63.071429\n",
      "salesman         35.666667\n",
      "scientist        35.548387\n",
      "student          22.081633\n",
      "technician       33.148148\n",
      "writer           36.311111\n",
      "Name: age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: Grouping (Users)\n",
    "#Get the dataset from this link and assign it to â€˜usersâ€™.\n",
    "#Note: You will continue working with â€˜usersâ€™ in XP Gold section.\n",
    "# Use the groupby() function to find the mean age per occupation.\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user\"\n",
    "users = pd.read_csv(url, sep='|')\n",
    "\n",
    "mean_age_per_occupation = users.groupby('occupation')['age'].mean()\n",
    "\n",
    "print(mean_age_per_occupation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43912d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data:\n",
      "   ID   Name\n",
      "0   1    One\n",
      "1   2    Two\n",
      "2   3  Three\n",
      "0   4   Four\n",
      "1   5   Five\n",
      "2   6    Six\n",
      "\n",
      "all_data_col:\n",
      "   ID   Name  ID  Name\n",
      "0   1    One   4  Four\n",
      "1   2    Two   5  Five\n",
      "2   3  Three   6   Six\n"
     ]
    }
   ],
   "source": [
    "# ðŸŒŸ Exercise 4: Merge (All_data, All_data_col, Data3)\n",
    "# Create three DataFrames based on the given raw data \n",
    "# (define what the raw data looks like and what data should be included in â€˜data1â€™, â€˜data2â€™, and â€˜data3â€™).\n",
    "# Note: These DataFrames will be used for merging exercises in the XP Gold section.\n",
    "    \n",
    "# Merge the two dataframes â€˜data1â€™ and â€˜data2â€™ along rows using the concat() function and assign it to â€˜all_dataâ€™.\n",
    "\n",
    "# Merge â€˜data1â€™ and â€˜data2â€™ along columns using concat() and assign it to â€˜all_data_colâ€™.\n",
    "\n",
    "data1_raw = {\n",
    "    'ID': [1, 2, 3],\n",
    "    'Name': ['One', 'Two', 'Three']\n",
    "}\n",
    "data1 = pd.DataFrame(data1_raw)\n",
    "\n",
    "data2_raw = {\n",
    "    'ID': [4, 5, 6],\n",
    "    'Name': ['Four', 'Five', 'Six']\n",
    "}\n",
    "data2 = pd.DataFrame(data2_raw)\n",
    "\n",
    "data3_raw = {\n",
    "    'ID': [7, 8, 9],\n",
    "    'Name': ['Seven', 'Eight', 'Nine']\n",
    "}\n",
    "data3 = pd.DataFrame(data3_raw)\n",
    "\n",
    "# Merge â€˜data1â€™ and â€˜data2â€™\n",
    "all_data = pd.concat([data1, data2])\n",
    "\n",
    "# Merge â€˜data1â€™ and â€˜data2â€™ along columns using concat\n",
    "all_data_col = pd.concat([data1, data2], axis=1)\n",
    "\n",
    "print(\"all_data:\")\n",
    "print(all_data)\n",
    "\n",
    "print(\"\\nall_data_col:\")\n",
    "print(all_data_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7894560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris:\n",
      "     sepal_length  sepal_width  petal_length  petal_width         species\n",
      "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
      "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
      "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
      "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
      "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
      "..            ...          ...           ...          ...             ...\n",
      "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
      "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
      "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
      "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
      "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "Missing values:\n",
      "\n",
      "sepal_length    False\n",
      "sepal_width     False\n",
      "petal_length    False\n",
      "petal_width     False\n",
      "species         False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# ðŸŒŸ Exercise 5: Deleting (Iris)\n",
    "# Retrieve the dataset from this link and assign it to â€˜irisâ€™.\n",
    "# Note: You will be manipulating the â€˜irisâ€™ DataFrame in the XP Gold section.\n",
    "\n",
    "# Assign appropriate column names to the DataFrame, such as \n",
    "# â€˜sepal_lengthâ€™, â€˜sepal_widthâ€™, â€˜petal_lengthâ€™, â€˜petal_widthâ€™, and â€˜speciesâ€™.\n",
    "\n",
    "# Check if there are any missing values in â€˜irisâ€™ using the isnull() function.\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "iris = pd.read_csv(url, names=column_names)\n",
    "\n",
    "missing_values = iris.isnull().any()\n",
    "\n",
    "print(\"iris:\")\n",
    "print(iris)\n",
    "\n",
    "print(\"Missing values:\\n\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f14a9456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         name   type  hp   evolution pokedex\n",
      "0   Bulbasaur  grass  45     Ivysaur     yes\n",
      "1  Charmander   fire  39  Charmeleon      no\n",
      "2    Squirtle  water  44   Wartortle     yes\n",
      "3    Caterpie    bug  45     Metapod      no\n"
     ]
    }
   ],
   "source": [
    "#Exercise 6: Creating Series And DataFrames (Pokemon)\n",
    "# Create a data dictionary with the following structure, then convert it into a DataFrame and name it â€˜pokemonâ€™:\n",
    "\n",
    "data = {\n",
    "    'evolution': ['Ivysaur', 'Charmeleon', 'Wartortle', 'Metapod'],\n",
    "    'hp': [45, 39, 44, 45],\n",
    "    'name': ['Bulbasaur', 'Charmander', 'Squirtle', 'Caterpie'],\n",
    "    'pokedex': ['yes', 'no', 'yes', 'no'],\n",
    "    'type': ['grass', 'fire', 'water', 'bug']\n",
    "}\n",
    "pokemon = pd.DataFrame(data)\n",
    "\n",
    "column_order = ['name', 'type', 'hp', 'evolution', 'pokedex']\n",
    "pokemon = pokemon[column_order]\n",
    "\n",
    "print(pokemon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a2899b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 entries:\n",
      "   Unnamed: 0     Id      Name  Year Gender State  Count\n",
      "0       11349  11350      Emma  2004      F    AK     62\n",
      "1       11350  11351   Madison  2004      F    AK     48\n",
      "2       11351  11352    Hannah  2004      F    AK     46\n",
      "3       11352  11353     Grace  2004      F    AK     44\n",
      "4       11353  11354     Emily  2004      F    AK     41\n",
      "5       11354  11355   Abigail  2004      F    AK     37\n",
      "6       11355  11356    Olivia  2004      F    AK     33\n",
      "7       11356  11357  Isabella  2004      F    AK     30\n",
      "8       11357  11358    Alyssa  2004      F    AK     29\n",
      "9       11358  11359    Sophia  2004      F    AK     28\n"
     ]
    }
   ],
   "source": [
    "# Exercise 7: Descriptive Statistics (Baby_names)\n",
    "\n",
    "# Start by downloading and loading the US Baby Names data into a DataFrame named baby_names.\n",
    "# Note: You will be working with â€˜baby_namesâ€™ in the XP Gold section.\n",
    "\n",
    "# Display the first 10 entries using the head method.\n",
    "\n",
    "# Delete the columns â€˜Unnamed: 0â€™ and â€˜Idâ€™ using the drop method.\n",
    "\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/06_Stats/US_Baby_Names/US_Baby_Names_right.csv\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception\n",
    "    data = StringIO(response.text)\n",
    "       \n",
    "    baby_names = pd.read_csv(data)\n",
    "    \n",
    "    print(\"First 10 entries:\")\n",
    "    print(baby_names.head(10))\n",
    "\n",
    "    #There was 404 before\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"The provided URL returned a 404 error. Please check the URL or try an alternative source.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c510ed25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First rows of investor_data:\n",
      "         Date  Total Equity  Domestic Equity  World Equity  Hybrid  \\\n",
      "0  2012-12-05         -7426            -6060         -1367     -74   \n",
      "1  2012-12-12         -8783            -7520         -1263     123   \n",
      "2  2012-12-19         -5496            -5470           -26     -73   \n",
      "3  2012-12-26         -4451            -4076          -375     550   \n",
      "4  2013-01-02        -11156            -9622         -1533    -158   \n",
      "\n",
      "   Total Bond  Taxable Bond  Municipal Bond  Total  \n",
      "0        5317          4210            1107  -2183  \n",
      "1        1818          1598             219  -6842  \n",
      "2         103          3472           -3369  -5466  \n",
      "3        2610          3333            -722  -1291  \n",
      "4        2383          2103             280  -8931  \n",
      "Frequency of the dataset: 7 days 00:00:00\n",
      "investor_data with 'Date' as index:\n",
      "            Total Equity  Domestic Equity  World Equity  Hybrid  Total Bond  \\\n",
      "Date                                                                          \n",
      "2012-12-05         -7426            -6060         -1367     -74        5317   \n",
      "2012-12-12         -8783            -7520         -1263     123        1818   \n",
      "2012-12-19         -5496            -5470           -26     -73         103   \n",
      "2012-12-26         -4451            -4076          -375     550        2610   \n",
      "2013-01-02        -11156            -9622         -1533    -158        2383   \n",
      "\n",
      "            Taxable Bond  Municipal Bond  Total  \n",
      "Date                                             \n",
      "2012-12-05          4210            1107  -2183  \n",
      "2012-12-12          1598             219  -6842  \n",
      "2012-12-19          3472           -3369  -5466  \n",
      "2012-12-26          3333            -722  -1291  \n",
      "2013-01-02          2103             280  -8931  \n"
     ]
    }
   ],
   "source": [
    "# Exercise 8: Handling Time Series Data (Investor_data)\n",
    "# Start by importing necessary libraries and load the Investor Flow of Funds US data.\n",
    "# Note: You will continue working with this â€˜investor_dataâ€™ DataFrame in XP Gold section.\n",
    "# Determine the frequency of the dataset.\n",
    "# Set â€˜Dateâ€™ as the index of the DataFrame using the set_index method.\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/datasets/investor-flow-of-funds-us/master/data/weekly.csv\"\n",
    "investor_data = pd.read_csv(url)\n",
    "\n",
    "print(\"First rows of investor_data:\")\n",
    "print(investor_data.head())\n",
    "\n",
    "investor_data['Date'] = pd.to_datetime(investor_data['Date'])\n",
    "\n",
    "frequency = investor_data['Date'].diff().mode()[0]\n",
    "print(\"Frequency of the dataset:\", frequency)\n",
    "\n",
    "investor_data.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"investor_data with 'Date' as index:\")\n",
    "print(investor_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
